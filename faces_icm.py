# -*- coding: utf-8 -*-
"""ICM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14yXZBEs7dEN-Rqwa2J1STjhvIIT9fDmE
"""

import torch
import os
import torch.nn as nn
import numpy as np
import time
from tensorboardX import SummaryWriter
from torchvision import datasets as datasets_torch
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
import importlib
import pandas as pd

torch_dtype = torch.float32
torch.set_default_dtype(torch_dtype)

# Model arguments
class Args:
  def __init__(self):
    self.input_size = 1
    self.seed = 11
    self.cuda = False
    self.name = ''
    self.dataset = 'faces'
    self.num_experts = 2
    self.batch_size = 32
    self.learning_rate_initialize = 1E-2
    self.learning_rate_expert = 1E-3
    self.learning_rate_discriminator = 1E-3
    self.epochs_init = 15
    self.epochs = 20
    self.optimizer_initialize = 'adam'
    self.optimizer_experts = 'adam'
    self.optimizer_discriminator = 'adam'
    self.outdir = '/home/vishnu/Documents/EngProjs/Masters/HC/'
    self.datadir = '/home/vishnu/Documents/EngProjs/Masters/HC/data/'
    self.load_initialized_experts = False
    self.model_for_initialized_experts = 'faces_n_exp_2_bs_32_lri_0.01_lre_0.001_lrd_0.001_ei_15_e_20_oi_adam_oe_adam_oe_adam_1654399976'
    self.inference_experts = False
    self.model_for_trained_experts = self.model_for_initialized_experts
    self.device =  torch.device("cuda" if self.cuda else "cpu")
    self.weight_decay = 0
    self.log_interval = 10
    self.iterations = 10000

args = Args()

torch.manual_seed(args.seed)
torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed_all(args.seed)

# Experiment name
timestamp = str(int(time.time()))
if args.name == '':
    name = '{}_n_exp_{}_bs_{}_lri_{}_lre_{}_lrd_{}_ei_{}_e_{}_oi_{}_oe_{}_oe_{}_{}'.format(
        args.dataset, args.num_experts, args.batch_size, args.learning_rate_initialize,
        args.learning_rate_expert, args.learning_rate_discriminator, args.epochs_init,
        args.epochs, args.optimizer_initialize, args.optimizer_experts, args.optimizer_discriminator,
        timestamp)
    args.name = name
else:
    args.name = '{}_{}'.format(args.name, timestamp)
print('\nExperiment: {}\n'.format(args.name))

# Logging. To run: tensorboard --logdir <args.outdir>/logs
log_dir = os.path.join(args.outdir, 'logs')
if not os.path.exists(log_dir):
    os.mkdir(log_dir)
log_dir_exp = os.path.join(log_dir, args.name)
os.mkdir(log_dir_exp)
writer = SummaryWriter(log_dir=log_dir_exp)

# Directory for checkpoints
checkpt_dir = os.path.join(args.outdir, 'checkpoints')
if not os.path.exists(checkpt_dir):
    os.mkdir(checkpt_dir)

class FACESDataset(Dataset):
    def __init__(self):
        self.path_asian = "data/faces_asian.npy"
        self.path_white = "data/faces_white.npy"
        self.path_black = "data/faces_black.npy"
        self.data_asian = np.load(self.path_asian)
        data_white = np.load(self.path_white)
        data_black = np.load(self.path_black)
        self.data_other = np.concatenate((data_black, data_white))[:self.data_asian.shape[0]]

    def __getitem__(self, index):
        X_after = self.data_other[index]
        # randint = np.random.randint(self.data_asian.shape[0])
        # X_before = self.data_asian[randint]
        X_before = self.data_asian[index]
        X_after = (X_after - X_after.min())/(X_after.max() - X_after.min())
        X_before = (X_before - X_before.min())/(X_before.max() - X_before.min())
        X_before = torch.Tensor(X_before).to(dtype=torch_dtype).reshape(3, 28, 28)
        X_after = torch.Tensor(X_after).to(dtype=torch_dtype).reshape(3, 28, 28)
        return (X_before, X_after)

    def __len__(self):
        return self.data_other.shape[0]

if args.dataset == "faces":
  dataset_train = FACESDataset()


# Create Dataloader from dataset
data_train = DataLoader(
    dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=int(args.cuda), pin_memory=args.cuda
)

class Expert(nn.Module):

    def __init__(self, args):
        super(Expert, self).__init__()
        self.args = args

        # Architecture
        def blockConv(k, in_feat, out_feat, BN = True, sigmoid = False):
          layers = [nn.Conv2d(in_feat, out_feat, k, padding='same')]
          if BN:
            layers.append(nn.BatchNorm2d(out_feat))
          if sigmoid:
            layers.append(nn.Sigmoid())
          else:
            layers.append(nn.ReLU(0.2))
          return layers

        if self.args.dataset == 'faces':
            self.model = nn.Sequential(
                *blockConv(3, 3, 32),
                *blockConv(3, 32, 32),
                *blockConv(3, 32, 32),
                *blockConv(3, 32, 32),
                *blockConv(3, 32, 3, BN = False),
            )
        else:
            raise NotImplementedError

    def forward(self, input):
        output = self.model(input)
        return output

class VAEExpert(nn.Module):
    def __init__(self, args, latent_size = 64):
        super(VAEExpert, self).__init__()
        self.latent_mode = False
        self.args = args
        self.latent_size = latent_size
        self.MSE = torch.nn.MSELoss()
        self.KLD = torch.nn.KLDivLoss(reduction="batchmean")
        self.Z_mean = torch.nn.Linear(self.latent_size, self.latent_size)
        self.Z_log_var = torch.nn.Linear(self.latent_size, self.latent_size)

        def blockConv(k, in_feat, out_feat, BN = True, sigmoid = False):
          layers = [nn.Conv2d(in_feat, out_feat, k, stride=2)]
          if BN:
            layers.append(nn.BatchNorm2d(out_feat))
          if sigmoid:
            layers.append(nn.Sigmoid())
          else:
            layers.append(nn.ReLU(0.2))
          return layers


        def blockDeConv(k, in_feat, out_feat, BN = True, sigmoid = False):
            layers = [nn.ConvTranspose2d(in_feat, out_feat, k, stride=1)]
            if BN:
                layers.append(nn.BatchNorm2d(out_feat))
            if sigmoid:
                layers.append(nn.Sigmoid())
            else:
                layers.append(nn.ReLU(0.2))
            return layers

        self.encoder = nn.Sequential(
            *blockConv(3, 3, 32),
            *blockConv(3, 32, 64),
            *blockConv(3, 64, 64),
            nn.Flatten(),
            nn.Linear(256, self.latent_size),
            nn.ReLU(0.2),
        )

        # Expect input 8x8
        self.decoder = nn.Sequential(
            *blockDeConv(6, 1, 64),
            *blockDeConv(6, 64, 64),
            *blockDeConv(6, 64, 32),
            *blockDeConv(6, 32, 3)
        )

    def forward(self, X):
        compress = self.encoder(X)
        Zmean = self.Z_mean(compress)
        ZlogVar = self.Z_log_var(compress)
        epsilon = torch.normal(0, 1, size = Zmean.shape)
        sample = Zmean + torch.exp(0.5 * ZlogVar) + epsilon
        sample = sample.view(-1, 1, 8, 8)
        decompress = self.decoder(sample)
        return Zmean, ZlogVar, decompress

    def loss(self, Z, X):
        Zmean, ZlogVar, decompress = Z
        reconstruction_loss = self.MSE(decompress, X)
        KLLoss = torch.mean(-0.5 * torch.sum(1 + ZlogVar - Zmean ** 2 - ZlogVar.exp(), dim = 1), dim = 0)
        return reconstruction_loss + (0.0001 * KLLoss)


class Discriminator(nn.Module):

    def __init__(self, args):
        super(Discriminator, self).__init__()
        self.args = args

        def blockConv(k, in_feat, out_feat, sigmoid = False):
          layers = [nn.Conv2d(in_feat, out_feat, k, padding='same')]
          if sigmoid:
            layers.append(nn.Sigmoid())
          else:
            layers.append(nn.ReLU(0.2))
          return layers

        # Architecture
        if self.args.dataset == 'faces':
            self.model = nn.Sequential(
                *blockConv(3, 3, 16),
                *blockConv(3, 16, 16),
                *blockConv(3, 16, 32),
                nn.AvgPool2d(2, 2),
                *blockConv(3, 32, 32),
                *blockConv(3, 32, 64),
                nn.AvgPool2d(2, 2),
                *blockConv(3, 64, 64),
                *blockConv(3, 64, 64),
                nn.AvgPool2d(2, 2),
                nn.Flatten(),
                nn.Linear(576, 25),
                nn.ReLU(0.2),
                nn.Linear(25, 1),
                nn.Sigmoid()
            )
        else:
            raise NotImplementedError

    def forward(self, input):
        validity = self.model(input)
        return validity

def init_weights(model, path):
    pre_trained_dict = torch.load(path, map_location=lambda storage, loc: storage)
    for layer in pre_trained_dict.keys():
        model.state_dict()[layer].copy_(pre_trained_dict[layer])
    for param in model.parameters():
        param.requires_grad = True

def initialize_expert(epochs, expert, i, optimizer, loss, data_train, args, writer):
    print("Initializing expert [{}] as identity on preturbed data".format(i+1))
    expert.train()

    for epoch in range(epochs):
        total_loss = 0
        n_samples = 0
        for batch in data_train:
            x_canonical, x_transf = batch
            batch_size = x_canonical.size(0)
            n_samples += batch_size
            x_transf = x_transf.to(args.device)
            x_canonical = x_canonical.to(args.device)
            params = expert(x_transf)
            loss_rec = expert.loss(params, x_transf)
            # x_hat = expert(x_transf)
            # loss_rec = loss(x_hat, x_transf)
            total_loss += loss_rec.item()*batch_size
            optimizer.zero_grad()
            loss_rec.backward()
            optimizer.step()

        # Loss
        mean_loss = total_loss/n_samples
        print("initialization epoch [{}] expert [{}] loss {:.4f}".format(
            epoch+1, i+1, mean_loss))
        writer.add_scalar('expert_{}_initialization_loss'.format(
            i+1), mean_loss, epoch+1)
        # if mean_loss < 0.002:
        #     break

    torch.save(expert.state_dict(), checkpt_dir +
               '/{}_E_{}_init.pth'.format(args.name, i + 1))
    
def train_system(epoch, experts, discriminator, optimizers_E, optimizer_D, criterion, data_train, args, writer):
    Dcriterion, Ecriterion = criterion
    print("checkpoint 0")
    discriminator.train()
    print("checkpoint 1")
    for i, expert in enumerate(experts):
        expert.train()

    print("checkpoint 2")

    # Labels for canonical vs transformed samples
    canonical_label = 1
    transformed_label = 0

    # Keep track of losses
    total_loss_D_canon = 0
    total_loss_D_transformed = 0
    n_samples = 0
    total_loss_expert = [0 for i in range(len(experts))]
    total_samples_expert = [0 for i in range(len(experts))]
    expert_scores_D = [0 for i in range(len(experts))]
    expert_winning_samples_idx = [[] for i in range(len(experts))]

    dataset = [data for data in data_train]
    for itr in range(args.iterations):
      randIDX = np.random.randint(0, len(dataset), 1)[0]
      x_canon, x_transf = dataset[randIDX]
      n_samples = x_canon.shape[0]


      canon_out = discriminator(x_canon)
      loss_target = torch.ones(canon_out.shape)
      loss_D = Dcriterion(canon_out, loss_target)
      total_loss_D_canon = loss_D.item() * n_samples
      optimizer_D.zero_grad()
      loss_D.backward()
      
      scores = []
      loss_D_transformed = 0
      loss_target = loss_target.fill_(0)
      for expert in experts:
        _,_, rec = expert(x_transf)
        score = discriminator(rec)
        # score = discriminator(expert(x_transf))
        loss_D_transformed += Dcriterion(score, loss_target)
        scores.append(score)
      loss_D_transformed = loss_D_transformed / args.num_experts
      total_loss_D_transformed = loss_D_transformed.item() * n_samples
      loss_D_transformed.backward()
      optimizer_D.step()

      scores = torch.cat(scores, dim = 1)
      scores_argmax = torch.argmax(scores, axis=1, keepdims=True)
      for exp_i in range(len(experts)):
        wins = torch.nonzero((scores_argmax[:] == exp_i).to(torch.long).squeeze())
        if wins.shape[0] > 0:
          total_samples_expert[exp_i] += wins.shape[0]
          params = experts[exp_i](x_transf[wins].squeeze(1))
          expi_scores = discriminator(params[2])
          # expi_scores = discriminator(experts[exp_i](x_transf[wins].squeeze(1)))
          loss_target = torch.ones(expi_scores.shape)
          # loss_expi = Ecriterion(expi_scores, loss_target)
          loss_expi = experts[exp_i].loss(params, x_transf[wins].squeeze())
          total_loss_expert[exp_i] += loss_expi.item() * wins.shape[0]
          optimizers_E[exp_i].zero_grad()
          loss_expi.backward(retain_graph = True)
          optimizers_E[exp_i].step()
          expert_scores_D[exp_i] += expi_scores.squeeze().sum().item()
      
      # Logging
      epoch = itr
      mean_loss_D_generated = total_loss_D_transformed / n_samples
      mean_loss_D_canon = total_loss_D_canon / n_samples
      print("epoch [{}] loss_D_transformed {:.4f}".format(
        epoch + 1, mean_loss_D_generated))
      print("epoch [{}] loss_D_canon {:.4f}".format(
        epoch + 1, mean_loss_D_canon))
      writer.add_scalar('loss_D_canonical', mean_loss_D_canon, epoch + 1)
      writer.add_scalar('loss_D_transformed', mean_loss_D_generated, epoch + 1)
      for i in range(len(experts)):
        print("epoch [{}] expert [{}] n_samples {}".format(
          epoch + 1, i + 1, total_samples_expert[i]))
        writer.add_scalar('expert_{}_n_samples'.format(
          i + 1), total_samples_expert[i], epoch + 1)
        writer.add_text('expert_{}_winning_samples'.format(i + 1),
                        ":".join([str(j) for j in expert_winning_samples_idx[i]]), epoch + 1)
        if total_samples_expert[i] > 0:
          mean_loss_expert = total_loss_expert[i] / total_samples_expert[i]
          mean_expert_scores = expert_scores_D[i] / total_samples_expert[i]
          print("epoch [{}] expert [{}] loss {:.4f}".format(
            epoch + 1, i + 1, mean_loss_expert))
          print("epoch [{}] expert [{}] scores {:.4f}".format(
            epoch + 1, i + 1, mean_expert_scores))
          writer.add_scalar('expert_{}_loss'.format(
            i + 1), mean_loss_expert, epoch + 1)
          writer.add_scalar('expert_{}_scores'.format(
            i + 1), mean_expert_scores, epoch + 1)

# Models Experts
experts = [VAEExpert(args).to(args.device) for i in range(args.num_experts)]
# Losses
loss_initial = torch.nn.MSELoss(reduction='mean')
Dcriterion = torch.nn.BCELoss(reduction='mean')
Ecriterion = torch.nn.MSELoss(reduction='mean')

# Initialize Experts as approximately Identity on Transformed Data
for i, expert in enumerate(experts):
    if args.load_initialized_experts:
        path = os.path.join(checkpt_dir,
                            args.model_for_initialized_experts + '_E_{}_init.pth'.format(i+1))
        init_weights(expert, path)
    else:
        if args.optimizer_initialize == 'adam':
            optimizer_E = torch.optim.Adam(expert.parameters(), lr=args.learning_rate_initialize,
                                            weight_decay=args.weight_decay)
        else:
            raise NotImplementedError
        initialize_expert(args.epochs_init, expert, i,
                          optimizer_E, loss_initial, data_train, args, writer)

for i,expert in enumerate(experts):
  if args.inference_experts:
    path = os.path.join(checkpt_dir,
                        args.model_for_trained_experts + '_E_{}_init.pth'.format(i + 1))
    init_weights(expert, path)

# Model Desc
discriminator = Discriminator(args).to(args.device)
optimizers_E = []
for i in range(args.num_experts):
    if args.optimizer_experts == 'adam':
        optimizer_E = torch.optim.Adam(experts[i].parameters(), lr=args.learning_rate_expert,
                                        weight_decay=args.weight_decay)
    elif args.optimizer_experts == 'sgd':
        optimizer_E = torch.optim.SGD(experts[i].parameters(), lr=args.learning_rate_expert,
                                      weight_decay=args.weight_decay)
    else:
        raise NotImplementedError
    optimizers_E.append(optimizer_E)
if args.optimizer_discriminator == 'adam':
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=args.learning_rate_discriminator,
                                    weight_decay=args.weight_decay)
elif args.optimizer_discriminator == 'sgd':
    optimizer_D = torch.optim.SGD(discriminator.parameters(), lr=args.learning_rate_discriminator,
                                  weight_decay=args.weight_decay)


if not args.inference_experts:
  epoch = 1
  train_system(epoch, experts, discriminator, optimizers_E,
               optimizer_D, (Dcriterion, Ecriterion), data_train, args, writer)
  torch.save(discriminator.state_dict(), checkpt_dir +
             '/{}_D.pth'.format(args.name))
  for i in range(args.num_experts):
    torch.save(experts[i].state_dict(), checkpt_dir +
               '/{}_E_init_{}.pth'.format(args.name, i+1))


import matplotlib.pyplot as plt
from PIL import Image

data_set = data_train.dataset
n_samples = len(data_set)

sample_count = 5
outs = []
for i in range(sample_count):
  random_index = int(np.random.random()*n_samples)
  single_example = data_set[random_index]
  x_canonical, x_transf = single_example

  x_transf_cuda = x_transf.to(args.device).unsqueeze(0)
  out = [expert(x_transf_cuda)[2].cpu().squeeze().detach().numpy()
          for expert in experts]
  out = [item.reshape((28, 28, 3)) for item in out]
  pwidth = 3
  cval = 50
  out = [x_canonical.numpy().reshape((28, 28, 3)),
         x_transf.numpy().reshape((28, 28, 3))] + out
  # out = [np.pad(item, axes=(1,2), pad_width=pwidth, constant_values=cval)
          # for item in out
  # ]
  out_s = np.concatenate(out, axis = 1)
  outs.append(out_s)

sample = np.concatenate(outs, axis = 0)
sample = (sample * 255).astype(np.uint8)
img = Image.fromarray(sample)
img.show()
